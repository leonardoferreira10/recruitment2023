{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Module 1 - Implementing and training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment verification\n",
    "Start by confirming you have PyTorch, TorchVision and TensorBoard installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:06:59.345355306Z",
     "start_time": "2023-10-21T18:06:55.636458470Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## QUESTIONS - General autonomous driving questions\n",
    "In this part, some general questions about autonomous driving, both general and specific to formula student, are presented. You should read the relevant parts of the rulebook and beginner's guide to answer some of the questions. Feel free to use the internet.\n",
    "\n",
    "1. List some pros and cons of using a stereo camera versus LiDAR versus RADAR for perception. You can research examples from the industry on why do they use specific sensors and not others.\n",
    "\n",
    "\n",
    "Sensors in Autonomous Vehicles: Choosing the Best Approach\n",
    "In autonomous vehicles, the use of various sensor systems is common for environment perception. The most popular sensors include LiDAR, radar, and cameras. These sensor systems work together to provide a comprehensive view of the external world, creating a safety network.\n",
    "\n",
    "LiDAR:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Distance Accuracy: LiDAR is highly accurate in measuring distances, crucial for collision avoidance.\n",
    "\n",
    "Light Independence: It works well in various lighting conditions, including complete darkness.\n",
    "\n",
    "Fast Scanning: It can quickly create 3D point clouds, enabling real-time perception.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "High Cost: LiDAR sensors can be expensive, especially high-quality ones.\n",
    "\n",
    "Susceptible to Obstructions: Opaque objects can block the laser beam, causing blind spots.\n",
    "\n",
    "Less Color Information: It doesn't provide color information, limiting some applications.\n",
    "\n",
    "Cameras:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Low Cost: Cameras are relatively affordable compared to other options.\n",
    "\n",
    "High Resolution: They can capture detailed images, useful for obstacle detection and navigation.\n",
    "\n",
    "Color and Texture: Cameras capture color and texture information, beneficial for object recognition.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Lighting Condition Sensitivity: Performance can be limited in adverse lighting conditions, such as rain, snow, or intense sunlight.\n",
    "\n",
    "Complex Processing: Stereoscopic image processing can be computationally expensive and requires powerful hardware.\n",
    "\n",
    "Depth Challenge: Estimating depth accurately at long distances can be challenging.\n",
    "\n",
    "RADAR:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Works in All Weather Conditions: RADAR is robust and performs well in rain, snow, and fog.\n",
    "\n",
    "Long Range: It can detect objects at long distances, ideal for high-speed vehicle detection.\n",
    "\n",
    "Less Affected by Reflective Surfaces: RADAR is less sensitive to reflective surfaces than LiDAR.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Lower Spatial Resolution: Compared to cameras and LiDAR, RADAR has lower spatial resolution and doesn't provide detailed object shape information.\n",
    "\n",
    "Complex Interactions: Interpreting RADAR signals in scenarios with multiple objects can be complicated.\n",
    "\n",
    "Considerable Cost: RADAR sensors can still be expensive.\n",
    "Use Examples:\n",
    "\n",
    "Cameras:\n",
    "\n",
    "Tesla: Tesla uses cameras in advanced driver-assistance systems and its autopilot.\n",
    "\n",
    "Waymo: Waymo, a subsidiary of Alphabet and a leader in autonomous vehicles, combines cameras with other technologies.\n",
    "\n",
    "Mobileye: Intel's Mobileye provides advanced computer vision solutions and cameras for autonomous vehicles and driver-assistance systems.\n",
    "\n",
    "LiDAR:\n",
    "\n",
    "Velodyne Lidar: Velodyne is a leading LiDAR sensor manufacturer, supplying several automotive and autonomous technology companies.\n",
    "\n",
    "Luminar: Luminar focuses on developing LiDAR sensors and provides technology for autonomous vehicles.\n",
    "\n",
    "Aurora: Autonomous vehicle company Aurora uses LiDAR technology in its autonomous vehicles and transport systems.\n",
    "\n",
    "RADAR:\n",
    "\n",
    "Bosch: Bosch is one of the major manufacturers of RADAR sensors for the automotive industry, used in advanced driver-assistance systems.\n",
    "\n",
    "Continental: Continental provides automotive radar systems to various car manufacturers.\n",
    "\n",
    "Uber ATG (Advanced Technologies Group): Uber ATG used RADAR sensors in its autonomous vehicles before selling the autonomous vehicle division.\n",
    "\n",
    "In conclusion, the choice of the ideal sensor depends on the project's specific requirements, the available budget, and operational conditions. A common approach is to combine multiple sensors to create redundancy and maximize environmental perception. This strategy compensates for individual weaknesses and results in safer and more efficient autonomous vehicle systems.\n",
    "\n",
    "Extra Sensors:\n",
    "\n",
    "Ultrasonic:\n",
    "\n",
    "Advantages: Low cost, detection of obstacles at short distances, effective in parking maneuvers.\n",
    "\n",
    "Disadvantages: Limited range, does not provide color information, low resolution.\n",
    "\n",
    "Applications: Parking, detection of obstacles at short distances.\n",
    "\n",
    "Inertial Sensors (IMU):\n",
    "\n",
    "Advantages: Measures acceleration and rotation, useful for detecting changes in the vehicle's position and orientation.\n",
    "\n",
    "Disadvantages: Does not provide information about objects in the environment.\n",
    "\n",
    "Applications: Complement for navigation and control systems.\n",
    "\n",
    "Additional Video: https://www.youtube.com/watch?v=qbxx7dsVLkw&list=PLtuNXpGOPQ_aeLQNxB4rLzfb8uktPABU9&index=3\n",
    "\n",
    "\n",
    "2. Stereo cameras are capable of perceiving both color and depth for each pixel. These cameras can be bought plug-and-play solutions (for example Intel RealSense or StereoLabs ZED 2) or self-made using industrial cameras (for example Basler). Computing depth from multiple cameras requires processing, called \"depth estimation\", which is done onboard on the plug and play solutions. Which solution would you opt for if you had a small team with a short budget? Consider complexity, reliability and cost on your decision.\n",
    "\n",
    "Development of a \"Self-Made\" Stereo Vision Solution:\n",
    "\n",
    "Hardware Component Selection:\n",
    "\n",
    "Camera Selection: We need to choose high-quality stereo cameras that are compatible and have features such as proper synchronization and the ability to capture high-resolution images.\n",
    "\n",
    "Additional Sensor Selection: In addition to cameras, we may need additional sensors such as gyroscopes and accelerometers to improve depth estimation accuracy.\n",
    "\n",
    "Camera Calibration:\n",
    "Camera calibration is a critical process. This involves determining the intrinsic (camera properties) and extrinsic (relative position and orientation relationships) parameters of stereo cameras.\n",
    "Calibrating the lenses and ensuring that the cameras are correctly aligned is essential for obtaining accurate depth information.\n",
    "\n",
    "Image Acquisition:\n",
    "Implementation of a system to capture images from both stereo cameras simultaneously.\n",
    "Precise synchronization of the cameras to ensure that the images are aligned in time.\n",
    "\n",
    "Image Processing for Stereo Matching:\n",
    "Implementation of stereo matching algorithms to find correspondences between points in the left and right camera images.\n",
    "The disparity calculated from these correspondences is used to estimate depth.\n",
    "\n",
    "Depth Calibration:\n",
    "Calibrating the depth output to convert depth information into real-world units.\n",
    "\n",
    "Integration with the Application:\n",
    "Integration of the generated depth information with your application or system to meet project requirements.\n",
    "\n",
    "Optimization and Improvements:\n",
    "Optimization of stereo matching algorithms to improve accuracy and performance.\n",
    "\n",
    "Testing and Validation:\n",
    "Conducting rigorous tests to ensure that the system provides accurate and reliable depth information.\n",
    "\n",
    "Ongoing Maintenance:\n",
    "Addressing potential issues, software updates, and continuous system maintenance.\n",
    "\n",
    "Advantages of Plug-and-Play Solutions (Intel RealSense, StereoLabs Zed2):\n",
    "\n",
    "Ease of Use: These solutions are designed to be user-friendly, allowing you to get started quickly without the need to assemble a complex system.\n",
    "\n",
    "Pre-Calibration: Cameras come pre-calibrated, eliminating the need for manual calibration.\n",
    "\n",
    "Embedded Processing: Plug-and-play solutions include embedded processors that perform real-time depth processing, eliminating the need to implement stereo matching algorithms.\n",
    "\n",
    "APIs and Documentation: Well-documented APIs are provided for integration with your applications.\n",
    "\n",
    "Support and Updates: You receive support from the manufacturing company and regular firmware and software updates.\n",
    "\n",
    "If I had a small team with a limited budget, I would opt for plug-and-play solutions, such as Intel RealSense or StereoLabs Zed2, which already include depth estimation processing and offer better performance while consuming fewer resources. Although they may be slightly more expensive compared to industrial cameras, the reduction in development complexity and workload can offset this additional cost.\n",
    "\n",
    "\n",
    "3. In an autonomous car, monitorization and reaction to critical failures are essential to prevent uncontrolled behavior. According to the rulebook and the beginner's guide, what must happen if the car detects a camera and/or LiDAR malfunction? Select the correct option(s), mentioning the relevant rule(s) you found:\n",
    "    1. Play a sound using the TSAC.\n",
    "    2. Eject the processing computer.\n",
    "    3. Activate the EBS.\n",
    "    4. Send a text message to the officials notifying the issue.\n",
    "    5. Autonomously approach the ASR to perform a safe shutdown.\n",
    "\n",
    "Answer: 3\n",
    "\n",
    "\"Concerning the high-level parts of the AS that rely on a variety of different sensor inputs,the system shall detect,if any of those is malfunctioning. If the proper vehicle operation cannot be ensured (e.g. loss of environmental perception) the system shall react by activating the EBS immediately.\"\n",
    "\n",
    "From: FSG23_AS_Beginners_Guide_v1.1.pdf\n",
    "\n",
    "4. Usually an autonomous driving pipeline is divided into perception, planning and control. Which algorithms are most commonly used by formula student teams on each of these stages? You can research other teams' social media or FSG Academy, for example.\n",
    "\n",
    "Perception: MLP, CNN, Image Processing Algorithms, RANSAC, EKF, AHRS\n",
    "\n",
    "Planning: Trajectory Planning Algorithms, SLAM\n",
    "\n",
    "Control: PID Controllers and Model-Based Control, EKF\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "The used dataset is the well-known MNIST, which is composed of images of handwritten digits (0 to 9) with 28 pixels wide and 28 pixels high.\n",
    "\n",
    "The goals of most of the models using this dataset is to classify the digit of the image, which is our case.\n",
    "\n",
    "Download the training and validation dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "training_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "validation_set: torch.utils.data.Dataset = torchvision.datasets.MNIST(\"./data\", train=False, download=True, transform=torchvision.transforms.ToTensor())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:07:05.964983173Z",
     "start_time": "2023-10-21T18:07:05.714450117Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 1 - MLP evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import the example MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-10-20T11:55:36.502285558Z"
    }
   },
   "outputs": [],
   "source": [
    "from bobnet import BobNet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an instance of this model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model1 = BobNet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-10-20T11:55:36.502581049Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the hyperparameters for this model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch size\n",
    "MLP_BATCH_SIZE=64\n",
    "\n",
    "# learning rate\n",
    "MLP_LEARNING_RATE=0.001\n",
    "\n",
    "# momentum\n",
    "MLP_MOMENTUM=0.9\n",
    "\n",
    "# training epochs to run\n",
    "MLP_EPOCHS=10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-10-20T11:55:36.502946001Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create the training and validation dataloaders from the datasets downloaded earlier:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the training loader\n",
    "mlp_training_loader = DataLoader(training_set, batch_size=MLP_BATCH_SIZE, shuffle=True) \n",
    "\n",
    "# create the validation loader\n",
    "mlp_validation_loader = DataLoader(validation_set, batch_size=MLP_BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-10-20T11:55:36.557116502Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the loss function and the optimizer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mlp_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_optimizer = torch.optim.SGD(model1.parameters(), lr=MLP_LEARNING_RATE, momentum=MLP_MOMENTUM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "start_time": "2023-10-20T11:55:36.557900716Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the training and validation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# how many batches between logs\n",
    "LOGGING_INTERVAL=100\n",
    "\n",
    "utils.train_model(model1, MLP_EPOCHS, mlp_optimizer, mlp_loss_fn, mlp_training_loader, mlp_validation_loader, LOGGING_INTERVAL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-20T11:55:36.560919243Z",
     "start_time": "2023-10-20T11:55:36.559184291Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QUESTIONS\n",
    "Explore the architecture on the script `mod1/bobnet.py`.\n",
    "1. Why does the input layer have 784 inputs? Consider the MNIST dataset samples' characteristics.\n",
    "\n",
    "The input layer has 784 units because the MNIST dataset consists of images that are 28 pixels wide and 28 pixels high, and if you multiply them, it results in 784.\n",
    "\n",
    "2. Why does the output layer have 10 outputs?\n",
    "\n",
    "The output layer has 10 outputs because MNIST has 10 output classes. In other words, the goal of MNIST is to evaluate images, so we use these images as input parameters, and the output will be the highest classification from 0 to 9."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2 - CNN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Head over to the `cnn.py` file and implement a convolutional architecture (add some convolutional layers and fully connected layers). You can search the LeNet architecture or AlexNet to get some insights and/or inspiration (you can implement a simpler version: with less layers). 2D convolutional layers in PyTorch are created using the `torch.nn.Conv2d` class. Activation and loss functions can be found under `torch.nn.functional` (like ReLU and softmax)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "from cnn import CNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:07:12.382480872Z",
     "start_time": "2023-10-21T18:07:12.308345353Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model2 = CNN()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:07:15.533267906Z",
     "start_time": "2023-10-21T18:07:15.447964725Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# batch size\n",
    "MLP_BATCH_SIZE=64\n",
    "\n",
    "# learning rate\n",
    "MLP_LEARNING_RATE=0.001\n",
    "\n",
    "# momentum\n",
    "MLP_MOMENTUM=0.9\n",
    "\n",
    "# training epochs to run\n",
    "MLP_EPOCHS=10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:07:18.192131570Z",
     "start_time": "2023-10-21T18:07:18.123786106Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# create the training loader\n",
    "mlp_training_loader = DataLoader(training_set, batch_size=MLP_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# create the validation loader\n",
    "mlp_validation_loader = DataLoader(validation_set, batch_size=MLP_BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:07:21.456199891Z",
     "start_time": "2023-10-21T18:07:21.382972636Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "mlp_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_optimizer = torch.optim.SGD(model2.parameters(), lr=MLP_LEARNING_RATE, momentum=MLP_MOMENTUM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:07:24.508868284Z",
     "start_time": "2023-10-21T18:07:24.432633438Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 (99/938): training_loss = 2.3260142995853617\n",
      "Epoch 0 (199/938): training_loss = 2.3143177188221533\n",
      "Epoch 0 (299/938): training_loss = 2.31039525035233\n",
      "Epoch 0 (399/938): training_loss = 2.3084390940224018\n",
      "Epoch 0 (499/938): training_loss = 2.3072835438714954\n",
      "Epoch 0 (599/938): training_loss = 2.3064856855618534\n",
      "Epoch 0 (699/938): training_loss = 2.3059213274026633\n",
      "Epoch 0 (799/938): training_loss = 2.3054729301729546\n",
      "Epoch 0 (899/938): training_loss = 2.305136476130586\n",
      "Epoch 0 (99/157): validation_loss = 2.325821876525879\n",
      "Epoch 1 (99/938): training_loss = 2.3257626114469585\n",
      "Epoch 1 (199/938): training_loss = 2.3140117499097506\n",
      "Epoch 1 (299/938): training_loss = 2.31007855951188\n",
      "Epoch 1 (399/938): training_loss = 2.308179300829283\n",
      "Epoch 1 (499/938): training_loss = 2.30698635678492\n",
      "Epoch 1 (599/938): training_loss = 2.3061850882929833\n",
      "Epoch 1 (699/938): training_loss = 2.305616382536117\n",
      "Epoch 1 (799/938): training_loss = 2.3051924344445944\n",
      "Epoch 1 (899/938): training_loss = 2.304852497060519\n",
      "Epoch 1 (99/157): validation_loss = 2.3254623413085938\n",
      "Epoch 2 (99/938): training_loss = 2.3254320934565382\n",
      "Epoch 2 (199/938): training_loss = 2.31364987962809\n",
      "Epoch 2 (299/938): training_loss = 2.3097715648919044\n",
      "Epoch 2 (399/938): training_loss = 2.3078196532744215\n",
      "Epoch 2 (499/938): training_loss = 2.3066799793549198\n",
      "Epoch 2 (599/938): training_loss = 2.3058628856837253\n",
      "Epoch 2 (699/938): training_loss = 2.3053107261657715\n",
      "Epoch 2 (799/938): training_loss = 2.3048987815317434\n",
      "Epoch 2 (899/938): training_loss = 2.3045598938149525\n",
      "Epoch 2 (99/157): validation_loss = 2.325136661529541\n",
      "Epoch 3 (99/938): training_loss = 2.3250612779097124\n",
      "Epoch 3 (199/938): training_loss = 2.31340653453041\n",
      "Epoch 3 (299/938): training_loss = 2.3095138527477865\n",
      "Epoch 3 (399/938): training_loss = 2.3075118112683595\n",
      "Epoch 3 (499/938): training_loss = 2.306341284023736\n",
      "Epoch 3 (599/938): training_loss = 2.3055609796998495\n",
      "Epoch 3 (699/938): training_loss = 2.304972272744677\n",
      "Epoch 3 (799/938): training_loss = 2.304539869365764\n",
      "Epoch 3 (899/938): training_loss = 2.304184237894943\n",
      "Epoch 3 (99/157): validation_loss = 2.324702501296997\n",
      "Epoch 4 (99/938): training_loss = 2.324413446464924\n",
      "Epoch 4 (199/938): training_loss = 2.3127598451010547\n",
      "Epoch 4 (299/938): training_loss = 2.3089316736495613\n",
      "Epoch 4 (399/938): training_loss = 2.307005158044342\n",
      "Epoch 4 (499/938): training_loss = 2.305808907281421\n",
      "Epoch 4 (599/938): training_loss = 2.3050155743135634\n",
      "Epoch 4 (699/938): training_loss = 2.304443714444729\n",
      "Epoch 4 (799/938): training_loss = 2.303981588838694\n",
      "Epoch 4 (899/938): training_loss = 2.3036224613995917\n",
      "Epoch 4 (99/157): validation_loss = 2.3239049911499023\n",
      "Epoch 5 (99/938): training_loss = 2.3238226982078167\n",
      "Epoch 5 (199/938): training_loss = 2.312192640112872\n",
      "Epoch 5 (299/938): training_loss = 2.308321139485541\n",
      "Epoch 5 (399/938): training_loss = 2.306316620126404\n",
      "Epoch 5 (499/938): training_loss = 2.305060960964593\n",
      "Epoch 5 (599/938): training_loss = 2.304246621060252\n",
      "Epoch 5 (699/938): training_loss = 2.3036253841138192\n",
      "Epoch 5 (799/938): training_loss = 2.3031467952776015\n",
      "Epoch 5 (899/938): training_loss = 2.3027665869677825\n",
      "Epoch 5 (99/157): validation_loss = 2.3228962421417236\n",
      "Epoch 6 (99/938): training_loss = 2.3225469035331647\n",
      "Epoch 6 (199/938): training_loss = 2.3109284142153945\n",
      "Epoch 6 (299/938): training_loss = 2.3070243210298162\n",
      "Epoch 6 (399/938): training_loss = 2.304936989208212\n",
      "Epoch 6 (499/938): training_loss = 2.303689363724244\n",
      "Epoch 6 (599/938): training_loss = 2.3028362696875315\n",
      "Epoch 6 (699/938): training_loss = 2.3021689648280326\n",
      "Epoch 6 (799/938): training_loss = 2.3016540822159213\n",
      "Epoch 6 (899/938): training_loss = 2.3011651352064497\n",
      "Epoch 6 (99/157): validation_loss = 2.320460081100464\n",
      "Epoch 7 (99/938): training_loss = 2.3201276605779473\n",
      "Epoch 7 (199/938): training_loss = 2.3084173238457146\n",
      "Epoch 7 (299/938): training_loss = 2.3043677248683663\n",
      "Epoch 7 (399/938): training_loss = 2.3021662127702758\n",
      "Epoch 7 (499/938): training_loss = 2.300749949797361\n",
      "Epoch 7 (599/938): training_loss = 2.299684162330946\n",
      "Epoch 7 (699/938): training_loss = 2.298875210793404\n",
      "Epoch 7 (799/938): training_loss = 2.298133543346343\n",
      "Epoch 7 (899/938): training_loss = 2.297469412259451\n",
      "Epoch 7 (99/157): validation_loss = 2.3140716552734375\n",
      "Epoch 8 (99/938): training_loss = 2.3130607147409457\n",
      "Epoch 8 (199/938): training_loss = 2.30068490253621\n",
      "Epoch 8 (299/938): training_loss = 2.2959185467914596\n",
      "Epoch 8 (399/938): training_loss = 2.2929063148068307\n",
      "Epoch 8 (499/938): training_loss = 2.289989860836633\n",
      "Epoch 8 (599/938): training_loss = 2.2873214103144677\n",
      "Epoch 8 (699/938): training_loss = 2.2838787428810874\n",
      "Epoch 8 (799/938): training_loss = 2.278310685641178\n",
      "Epoch 8 (899/938): training_loss = 2.2699942225476395\n",
      "Epoch 8 (99/157): validation_loss = 2.195087432861328\n",
      "Epoch 9 (99/938): training_loss = 2.1855666541089915\n",
      "Epoch 9 (199/938): training_loss = 2.1587716814261584\n",
      "Epoch 9 (299/938): training_loss = 2.1376977058557363\n",
      "Epoch 9 (399/938): training_loss = 2.119969746821506\n",
      "Epoch 9 (499/938): training_loss = 2.0967193700985343\n",
      "Epoch 9 (599/938): training_loss = 2.0695782595365393\n",
      "Epoch 9 (699/938): training_loss = 2.0418372057026546\n",
      "Epoch 9 (799/938): training_loss = 2.015408584113712\n",
      "Epoch 9 (899/938): training_loss = 1.9915022205590407\n",
      "Epoch 9 (99/157): validation_loss = 1.7893965244293213\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(1.7684)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "# how many batches between logs\n",
    "LOGGING_INTERVAL=100\n",
    "\n",
    "utils.train_model(model2, MLP_EPOCHS, mlp_optimizer, mlp_loss_fn, mlp_training_loader, mlp_validation_loader, LOGGING_INTERVAL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-10-21T18:09:48.232964740Z",
     "start_time": "2023-10-21T18:07:27.903122872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:11:08.822590734Z",
     "start_time": "2023-10-21T18:11:08.715979089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Caminho do modelo salvo\n",
    "save_path = 'cnn_treinado.pth'\n",
    "\n",
    "# Salve o modelo\n",
    "torch.save(model2.state_dict(), save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-21T18:13:52.856749331Z",
     "start_time": "2023-10-21T18:13:52.654672053Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### QUESTIONS\n",
    "\n",
    "1. What are the advantages of using convolutional layers versus fully-connected layers for image processing?\n",
    "\n",
    "\n",
    "Parameter Sharing: Convolutional layers share weights, reducing parameters, while fully connected layers have many parameters, leading to overfitting.\n",
    "\n",
    "Spatial Hierarchy: Convolutional layers capture a hierarchy of features, from low to high-level, preserving spatial structure. Fully connected layers do not preserve spatial structure.\n",
    "\n",
    "Translation Invariance: Convolutional layers are translation-invariant, being robust to position changes. Fully connected layers do not possess this property.\n",
    "\n",
    "Efficiency: Convolutional layers are efficient, reusing weights, while fully connected layers can be computationally expensive, especially with large images.\n",
    "\n",
    "Local Receptive Fields: Convolutional layers use local receptive fields, capturing local details. Fully connected layers are not as effective in this regard.\n",
    "\n",
    "Feature Hierarchies: CNNs learn feature hierarchies, useful for image processing tasks. Fully connected layers do not have this advantage.\n",
    "\n",
    "In conclusion, convolutional layers are suitable for image processing tasks because they leverage the spatial structure of images, reduce the number of parameters, and effectively capture local and hierarchical features. Fully connected layers are often used in conjunction with convolutional layers for end-to-end learning tasks in neural networks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
